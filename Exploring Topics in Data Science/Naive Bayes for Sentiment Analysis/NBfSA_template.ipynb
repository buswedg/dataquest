{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Exploring Topics in Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Naive Bayes for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###1: Before we classify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a csv file containing movie reviews. Each row in the dataset contains the text of the review, and whether the tone of the review was classified as positive(1), or negative(-1).\n",
    "\n",
    "We want to predict whether a review is negative or positive given only the text. In order to do this, we'll train an algorithm using the reviews and classifications in train.csv, and then make predictions on the reviews in test.csv. We'll then be able to calculate our error using the actual classifications in test.csv, and see how good our predictions were.\n",
    "\n",
    "For our classification algorithm, we're going to use <a href = \"http://en.wikipedia.org/wiki/Naive_Bayes_classifier\">naive bayes</a>. A naive bayes classifier works by figuring out the probability of different attributes of the data being associated with a certain class. This is based on <a href = \"http://en.wikipedia.org/wiki/Bayes%27_theorem\">bayes' theorem</a>. The theorem is $P(A \\mid B) = \\frac{P(B \\mid A) \\, P(A)}{P(B)}$. This basically states \"the probability of A given that B is true equals the probability of B given that A is true times the probability of A being true, divided by the probability of B being true.\"\n",
    "\n",
    "Let's do a quick exercise to understand this rule better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import glob\n",
    "\n",
    "count = 0\n",
    "review = []\n",
    "reviews_neg = []\n",
    "path = \"data/train/neg/\"\n",
    "for infile in glob.glob( os.path.join(path, '*.*') ):\n",
    "    if count < 1000:\n",
    "        #print(\"current file is: \" + infile)\n",
    "        with open(infile, encoding=\"utf8\") as f:\n",
    "            test = f.read()\n",
    "            review = [test, -1]\n",
    "            reviews_neg.append(review)\n",
    "    count += 1\n",
    "\n",
    "count = 0\n",
    "review = []\n",
    "reviews_pos = []\n",
    "path = 'data/train/pos/'\n",
    "for infile in glob.glob( os.path.join(path, '*.*') ):\n",
    "    if count < 1000:\n",
    "        #print(\"current file is: \" + infile)\n",
    "        with open(infile, encoding=\"utf8\") as f:\n",
    "            test = f.read()\n",
    "            review = [test, 1]\n",
    "            reviews_pos.append(review)\n",
    "    count += 1\n",
    "\n",
    "\n",
    "reviews = reviews_neg + reviews_pos\n",
    "\n",
    "with open(\"data/train.csv\", \"w\", newline='', encoding=\"utf8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(reviews)\n",
    "    \n",
    "    \n",
    "count = 0\n",
    "review = []\n",
    "reviews_neg = []\n",
    "path = 'data/test/neg/'\n",
    "for infile in glob.glob( os.path.join(path, '*.*') ):\n",
    "    if count < 1000:\n",
    "        #print(\"current file is: \" + infile)\n",
    "        with open(infile, encoding=\"utf8\") as f:\n",
    "            test = f.read()\n",
    "            review = [test, -1]\n",
    "            reviews_neg.append(review)\n",
    "    count += 1\n",
    "\n",
    "count = 0\n",
    "review = []\n",
    "reviews_pos = []\n",
    "path = 'data/test/pos/'\n",
    "for infile in glob.glob( os.path.join(path, '*.*') ):\n",
    "    if count < 1000:\n",
    "        #print(\"current file is: \" + infile)\n",
    "        with open(infile, encoding=\"utf8\") as f:\n",
    "            test = f.read()\n",
    "            review = [test, 1]\n",
    "            reviews_pos.append(review)\n",
    "    count += 1\n",
    "\n",
    "\n",
    "reviews = reviews_neg + reviews_pos\n",
    "\n",
    "with open(\"data/test.csv\", \"w\", newline='', encoding=\"utf8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative_text[:100]: story of a man who has unnatural feelings for a pig. starts out with a opening scene that is a terri\n",
      "positive_text[:100]: bromwell high is a cartoon comedy. it ran at the same time as some other programs about school life,\n"
     ]
    }
   ],
   "source": [
    "# A nice python class that lets you count how many times items occur in a list\n",
    "from collections import Counter\n",
    "import csv\n",
    "import re\n",
    "\n",
    "# Read in the training data.\n",
    "with open(\"data/train.csv\", 'r', encoding=\"utf8\") as file:\n",
    "    reviews = list(csv.reader(file))\n",
    "\n",
    "count = 0\n",
    "for row in reviews:\n",
    "    #if count >= 10:\n",
    "        #print(reviews[count])\n",
    "    count += 1\n",
    "\n",
    "def get_text(reviews, score):\n",
    "    # Join together the text in the reviews for a particular tone.\n",
    "    # We lowercase to avoid \"Not\" and \"not\" being seen as different words, for example.\n",
    "    return \" \".join([r[0].lower() for r in reviews if r[1] == str(score)])\n",
    "\n",
    "def count_text(text):\n",
    "    # Split text into words based on whitespace.  Simple but effective.\n",
    "    words = re.split(\"\\s+\", text)\n",
    "    # Count up the occurence of each word.\n",
    "    return Counter(words)\n",
    "\n",
    "negative_text = get_text(reviews, -1)\n",
    "positive_text = get_text(reviews, 1)\n",
    "# Generate word counts for negative tone.\n",
    "negative_counts = count_text(negative_text)\n",
    "# Generate word counts for positive tone.\n",
    "positive_counts = count_text(positive_text)\n",
    "\n",
    "print(\"negative_text[:100]:\", negative_text[:100])\n",
    "print(\"positive_text[:100]:\", positive_text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###2: Naive bayes intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a slightly different example. Let's say we still had one classification -- whether or not you were tired. And let's say we had two data points -- whether or not you ran, and whether or not you woke up early. Bayes' theorem doesn't work in this case, because we have two data points, not just one.\n",
    "\n",
    "This is where naive bayes can help. Naive bayes extends bayes' theorem to handle this case by assuming that each data point is independent.\n",
    "\n",
    "The formula looks like this: $P(y \\mid x_1, \\dots, x_n) = \\frac{P(y) \\prod_{i=1}^{n} P(x_i \\mid y)}{P(x_1, \\dots, x_n)}$. This is saying \"the probability that classification y is correct given the features $x_1$, $x_2$, and so on equals the probability of y times the product of each x feature given y, divided by the probability of the x features\".\n",
    "\n",
    "To find the \"right\" classification, we just find out which classification $P(y \\mid x_1, \\dots, x_n)$ has the highest probability with the formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final classification for new day: was tired. Tired probability: 0.10204081632653061. Not tired probability: 0.054421768707482984.\n"
     ]
    }
   ],
   "source": [
    "# Here's our data, but with \"woke up early\" or \"didn't wake up early\" added.\n",
    "days = [[\"ran\", \"was tired\", \"woke up early\"], [\"ran\", \"was not tired\", \"didn't wake up early\"], [\"didn't run\", \"was tired\", \"woke up early\"], [\"ran\", \"was tired\", \"didn't wake up early\"], [\"didn't run\", \"was tired\", \"woke up early\"], [\"ran\", \"was not tired\", \"didn't wake up early\"], [\"ran\", \"was tired\", \"woke up early\"]]\n",
    "\n",
    "# We're trying to predict whether or not the person was tired on this day.\n",
    "new_day = [\"ran\", \"didn't wake up early\"]\n",
    "\n",
    "def calc_y_probability(y_label, days):\n",
    "    return len([d for d in days if d[1] == y_label]) / len(days)\n",
    "\n",
    "def calc_ran_probability_given_y(ran_label, y_label, days):\n",
    "    return len([d for d in days if d[1] == y_label and d[0] == ran_label]) / len(days)\n",
    "\n",
    "def calc_woke_early_probability_given_y(woke_label, y_label, days):\n",
    "    return len([d for d in days if d[1] == y_label and d[2] == woke_label]) / len(days)\n",
    "\n",
    "denominator = len([d for d in days if d[0] == new_day[0] and d[2] == new_day[1]]) / len(days)\n",
    "# Plug all the values into our formula.  Multiply the class (y) probability, and the probability of the x-values occuring given that class.\n",
    "prob_tired = (calc_y_probability(\"was tired\", days) * calc_ran_probability_given_y(new_day[0], \"was tired\", days) * calc_woke_early_probability_given_y(new_day[1], \"was tired\", days)) / denominator\n",
    "\n",
    "prob_not_tired = (calc_y_probability(\"was not tired\", days) * calc_ran_probability_given_y(new_day[0], \"was not tired\", days) * calc_woke_early_probability_given_y(new_day[1], \"was not tired\", days)) / denominator\n",
    "\n",
    "# Make a classification decision based on the probabilities.\n",
    "classification = \"was tired\"\n",
    "if prob_not_tired > prob_tired:\n",
    "    classification = \"was not tired\"\n",
    "    \n",
    "print(\"Final classification for new day: {0}. Tired probability: {1}. Not tired probability: {2}.\".format(classification, prob_tired, prob_not_tired))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###3: Finding word counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're trying to determine if a data row should be classified as negative or positive. Because of this, we can ignore the denominator. As you saw in the last code example, it will be a constant in each of the possible classes, thus affecting each probability equally, so it won't change which one is greatest (dividing 5 by 2 and 10 by 2 doesn't change the fact that the second number is bigger).\n",
    "\n",
    "So we have to calculate the probabilities of each classification, and the probabilities of each feature falling into each classification.\n",
    "\n",
    "We were working with several discrete features in the last example. Here, all we have is one long string. The easiest way to generate features from text is to split the text up into words. Each word in a review will then be a feature that we can then work with. In order to do this, we'll split the reviews based on whitespace.\n",
    "\n",
    "We'll then count up how many times each word occurs in the negative reviews, and how many times each word occurs in the positive reviews. This will allow us to eventually compute the probabilities of a new review belonging to each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative_text[:100]: story of a man who has unnatural feelings for a pig. starts out with a opening scene that is a terri\n",
      "positive_text[:100]: bromwell high is a cartoon comedy. it ran at the same time as some other programs about school life,\n"
     ]
    }
   ],
   "source": [
    "# A nice python class that lets you count how many times items occur in a list\n",
    "from collections import Counter\n",
    "import csv\n",
    "import re\n",
    "\n",
    "# Read in the training data.\n",
    "with open(\"data/train.csv\", 'r', encoding=\"utf8\") as file:\n",
    "    reviews = list(csv.reader(file))\n",
    "\n",
    "def get_text(reviews, score):\n",
    "    # Join together the text in the reviews for a particular tone.\n",
    "    # We lowercase to avoid \"Not\" and \"not\" being seen as different words, for example.\n",
    "    return \" \".join([r[0].lower() for r in reviews if r[1] == str(score)])\n",
    "\n",
    "def count_text(text):\n",
    "    # Split text into words based on whitespace.  Simple but effective.\n",
    "    words = re.split(\"\\s+\", text)\n",
    "    # Count up the occurence of each word.\n",
    "    return Counter(words)\n",
    "\n",
    "negative_text = get_text(reviews, -1)\n",
    "positive_text = get_text(reviews, 1)\n",
    "# Generate word counts for negative tone.\n",
    "negative_counts = count_text(negative_text)\n",
    "# Generate word counts for positive tone.\n",
    "positive_counts = count_text(positive_text)\n",
    "\n",
    "print(\"negative_text[:100]:\", negative_text[:100])\n",
    "print(\"positive_text[:100]:\", positive_text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###4: Making predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the word counts, we just have to convert them to probabilities and multiply them out to get the predicted classification. Let's say we wanted to find the probability that the review didn't like it expresses a negative sentiment. We would find the total number of times the word didn't occured in the negative reviews, and divide it by the total number of words in the negative reviews to get the probability of x given y. We would then do the same for like and it. We would multiply all three probabilities, and then multiply by the probability of any document expressing a negative sentiment to get our final probability that the sentence expresses negative sentiment.\n",
    "\n",
    "We would do the same for positive sentiment, and then whichever probability is greater would be the class that the review is assigned to.\n",
    "\n",
    "To do all this, we'll need to compute the probabilities of each class occuring in the data, and then make a function to compute the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reviews[0][0]: Story of a man who has unnatural feelings for a pig. Starts out with a opening scene that is a terrific example of absurd comedy. A formal orchestra audience is turned into an insane, violent mob by the crazy chantings of it's singers. Unfortunately it stays absurd the WHOLE time with no general narrative eventually making it just too off putting. Even those from the era should be turned off. The cryptic dialogue would make Shakespeare seem easy to a third grader. On a technical level it's better than you might think with some good cinematography by future great Vilmos Zsigmond. Future stars Sally Kirkland and Frederic Forrest can be seen briefly.\n",
      "neg_pred: 0.0\n",
      "pos_pred: 0.0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def get_y_count(score):\n",
    "    # Compute the count of each classification occuring in the data.\n",
    "    return len([r for r in reviews if r[1] == str(score)])\n",
    "\n",
    "# We need these counts to use for smoothing when computing the prediction.\n",
    "positive_review_count = get_y_count(1)\n",
    "negative_review_count = get_y_count(-1)\n",
    "\n",
    "# These are the class probabilities (we saw them in the formula as P(y)).\n",
    "prob_positive = positive_review_count / len(reviews)\n",
    "#print(prob_positive)\n",
    "prob_negative = negative_review_count / len(reviews)\n",
    "#print(prob_negative)\n",
    "\n",
    "def make_class_prediction(text, counts, class_prob, class_count):\n",
    "    prediction = 1\n",
    "    text_counts = Counter(re.split(\"\\s+\", text))\n",
    "    for word in text_counts:\n",
    "        # For every word in the text, we get the number of times that word occured in the reviews for a given class, add 1 to smooth the value, and divide by the total number of words in the class (plus the class_count to also smooth the denominator).\n",
    "        # Smoothing ensures that we don't multiply the prediction by 0 if the word didn't exist in the training data.\n",
    "        # We also smooth the denominator counts to keep things even.\n",
    "        prediction *=  float(text_counts.get(word) * ((counts.get(word, 0) + 1)) / (sum(counts.values()) + class_count))\n",
    "    # Now we multiply by the probability of the class existing in the documents.\n",
    "    return prediction * class_prob\n",
    "\n",
    "# As you can see, we can now generate probabilities for which class a given review is part of.\n",
    "# The probabilities themselves aren't very useful -- we make our classification decision based on which value is greater.\n",
    "print(\"reviews[0][0]:\", reviews[0][0])\n",
    "\n",
    "neg_pred = make_class_prediction(reviews[0][0], negative_counts, prob_negative, negative_review_count)\n",
    "print(\"neg_pred:\", neg_pred)\n",
    "\n",
    "pos_pred = make_class_prediction(reviews[0][0], positive_counts, prob_positive, positive_review_count)\n",
    "print(\"pos_pred:\", pos_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###5: Predicting the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can make predictions, let's predict the probabilities on the reviews in test.csv. You'll get misleadingly good results if you predict on the reviews in train.csv, because the probabilities were generated from it (and this, the algorithm has prior knowledge about the data it's predicting on).\n",
    "\n",
    "Getting good results on the training set could mean that your model is <a href = \"http://en.wikipedia.org/wiki/Overfitting\">overfit</a>, and is just picking up random noise. Only testing on a set that the model wasn't trained with can tell you if it's performing properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions[:5]: [1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def make_decision(text, make_class_prediction):\n",
    "    # Compute the negative and positive probabilities.\n",
    "    negative_prediction = make_class_prediction(text, negative_counts, prob_negative, negative_review_count)\n",
    "    positive_prediction = make_class_prediction(text, positive_counts, prob_positive, positive_review_count)\n",
    "\n",
    "    # We assign a classification based on which probability is greater.\n",
    "    if negative_prediction > positive_prediction:\n",
    "        return -1\n",
    "    return 1\n",
    "\n",
    "with open(\"data/test.csv\", \"r\", encoding=\"utf8\") as file:\n",
    "    test = list(csv.reader(file))\n",
    "\n",
    "predictions = [make_decision(r[0], make_class_prediction) for r in test]\n",
    "print(\"predictions[:5]:\", predictions[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###6: Computing error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know the predictions, we'll compute error using the area under the <a href = \"http://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_curve\">ROC curve</a>. This will tell us how \"good\" the model is -- closer to 1 means that the model is better.\n",
    "\n",
    "Computing error is very important to knowing when your model is \"good\", and when it is getting better or worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.559\n"
     ]
    }
   ],
   "source": [
    "actual = [int(r[1]) for r in test]\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "# Generate the roc curve using scikits-learn.\n",
    "fpr, tpr, thresholds = metrics.roc_curve(actual, predictions, pos_label=1)\n",
    "\n",
    "# Measure the area under the curve.  The closer to 1, the \"better\" the predictions.\n",
    "print(\"AUC:\", metrics.auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###7: A faster way to predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of extensions that we could make to this algorithm to make it perform better. We could look at <a href = \"http://en.wikipedia.org/wiki/N-gram\">n-grams</a> instead of unigrams. We could remove punctuation and other non-characters. We could remove <a href = \"http://en.wikipedia.org/wiki/Stop_words\">stopwords</a>. We could also perform <a href = \"http://en.wikipedia.org/wiki/Stemming\">stemming</a> or lemmatization.\n",
    "\n",
    "We don't want to have to code the whole algorithm out every time, though. An easier way to use naive bayes is to use the implementation in scikit-learn. <a href = \"http://scikit-learn.org/stable/\">Scikit-learn</a> is a Python machine learning library that contains implementations of all the common machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.8325\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics\n",
    "\n",
    "# Generate counts from text using a vectorizer.  There are other vectorizers available, and lots of options you can set.\n",
    "# This performs our step of computing word counts.\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "train_features = vectorizer.fit_transform([r[0] for r in reviews])\n",
    "test_features = vectorizer.transform([r[0] for r in test])\n",
    "\n",
    "# Fit a naive bayes model to the training data.\n",
    "# This will train the model using the word counts we compute, and the existing classifications in the training set.\n",
    "nb = MultinomialNB()\n",
    "nb.fit(train_features, [int(r[1]) for r in reviews])\n",
    "\n",
    "# Now we can use the model to predict classifications for our test features.\n",
    "predictions = nb.predict(test_features)\n",
    "\n",
    "# Compute the error. It is slightly different from our model because the internals of this process work differently from our implementation.\n",
    "fpr, tpr, thresholds = metrics.roc_curve(actual, predictions, pos_label=1)\n",
    "print(\"AUC:\", metrics.auc(fpr, tpr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
